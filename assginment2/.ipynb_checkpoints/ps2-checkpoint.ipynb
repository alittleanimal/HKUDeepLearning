{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "clear-seven",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeea200157319b12930300a5b5465bb1",
     "grade": false,
     "grade_id": "des",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: text classification.\n",
    "In this notebook assignment, you will use **Tensorflow 2** to build a deep neural network for a simple text classification task. Specifically, you will\n",
    "- build a simple LSTM network with basic **numpy** functions;\n",
    "- trains a sentiment analysis model to classify movie reviews [IMDB](https://ai.stanford.edu/%7Eamaas/data/sentiment/) as positive or negative, based on the text of the review. You will implement a deep neural network, with the advantage of pre-trained word embedding and BERT model.\n",
    "\n",
    "## Important!!\n",
    "- Please finish Problem 1 to 4. Fill the codes between **STAET CODE HERE** and **END CODE HERE**.\n",
    "- Make sure you pass the tester functions before your submission. Feel free to add cells to debug your codes.\n",
    "- **Please rename your sumited jupyter notebok with your student ID**. Otherwise, your work can not be scored accordingly.\n",
    "- You can add cells when you are preparing your assignment. DO **delete** them when you are ready to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distributed-finance",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a65538c255a834e63ac23379e2a9bc2",
     "grade": false,
     "grade_id": "installation",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip3 install -q tensorflow\n",
    "!pip3 install -q tensorflow-text\n",
    "!pip3 install -q tensorflow_datasets\n",
    "!pip3 install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "connected-movement",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "067a3c04224c8d41c8046b176a0fa3e0",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\"\n",
    "assert tf.__version__.split(\".\")[0] == \"2\", \"Your version of Tensorflow is not version 2.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-redhead",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f82af955031d5099db32355640383e82",
     "grade": false,
     "grade_id": "activation_des",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem :1 activation function (10 points) \n",
    "\n",
    "Implement **tanh** and **sigmoid** activatin function with **numpy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cheap-bishop",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d342a3cae799bf6e58356c65bd31096",
     "grade": false,
     "grade_id": "activation_tanh",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: a numpy array\n",
    "    Return:\n",
    "    s : a numpy array\n",
    "    \"\"\"\n",
    "    ## START CODE HERE ## (~ 1 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    s = np.tanh(x)\n",
    "    ## END CODE HERE ##\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spread-agency",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd2554e37790c0fd5db55920b0de30ee",
     "grade": false,
     "grade_id": "activation_sigmoid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: a numpy array\n",
    "    Return:\n",
    "    s : a numpy array\n",
    "    \"\"\"\n",
    "    ## START CODE HERE ## (~ 1 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    s = 1.0 / (1.0 + np.exp(-x))\n",
    "    ## END CODE HERE ##\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entitled-computer",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "713b7cb8a3ce1760c52858d7c212f01d",
     "grade": true,
     "grade_id": "activation_tanh_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "def test_tanh(x):\n",
    "    assert isinstance(x, np.ndarray) is True, \"x should be of type numpy.ndarray\"\n",
    "    print(\"Pass\")\n",
    "    return\n",
    "\n",
    "x = np.array([0.5], dtype=np.float)\n",
    "test_tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "instructional-pharmacy",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad16c43730f76ba685cfacf6960fbbf",
     "grade": true,
     "grade_id": "activation_sigmoid_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "def test_sigmoid(x):\n",
    "    assert isinstance(x, np.ndarray) is True, \"x should be of type numpy.ndarray\"\n",
    "    print(\"Pass\")\n",
    "    return\n",
    "x = np.array([0.5], dtype=np.float)\n",
    "test_sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-insertion",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fee6b4c6a1429f8ae4790cd0e6be7bc",
     "grade": false,
     "grade_id": "lstm_des",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 2: LSTM cell (40 points)\n",
    "LSTM Cell computes $c$, and $h$. $c$ is like the long-term memory, and $h$ is like the short term memory. We use the input $x$ and $h$ to update the long term memory. In the update, some features of $c$ are cleared with a forget gate $f$, and some features $i$ are added through a gate $g$.\n",
    "\n",
    "Here is the update rule:\n",
    "$$c_t = \\sigma(f_t) \\bigodot c_{t-1} + \\sigma(i_t) \\bigodot tanh(g_t)$$\n",
    "\n",
    "$$h_t = \\sigma(o_t) \\bigodot tanh(c_t)$$\n",
    "\n",
    "$\\bigodot$ stands for element-wise multiplication.\n",
    "\n",
    "Intermediate values and gates are computed as linear transformations of the hidden state and input.\n",
    "\n",
    "$$f_t = W_{xf} x_t + W_{hf} h_{t-1} + b_f$$\n",
    "\n",
    "$$i_t = W_{xi} x_t + W_{hi} h_{t-1} + b_i$$\n",
    "\n",
    "$$g_t = W_{xg} x_t + W_{hg} h_{t-1} + b_g$$\n",
    "\n",
    "$$o_t = W_{xo} x_t + W_{ho} h_{t-1} + b_o$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affecting-conservative",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "531eb633fd4c22611249760ef2c56946",
     "grade": false,
     "grade_id": "lstm_cell",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lstm_cell(x, h, c,\n",
    "              weight_xi, weight_hi, bias_i,\n",
    "              weight_xf, weight_hf, bias_f,\n",
    "              weight_xg, weight_hg, bias_g,\n",
    "              weight_xo, weight_ho, bias_o):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      x: a numpy array, input state\n",
    "      h: a numpy array, hidden state\n",
    "      c: a numpy array, context memory\n",
    "      weight_xi: a numpy array, weight for input gate\n",
    "      weight_hi: a numpy array, weight for input gate\n",
    "      bias_i: a numpy array, bias for input gate\n",
    "      weight_xf: a numpy array, weight for forget gate\n",
    "      weight_hf: a numpy array, weight for forget gate\n",
    "      bias_f: a numpy array, bias for forget gate\n",
    "      weight_xg: a numpy array, weight for g gate\n",
    "      weight_hg: a numpy array, weight for g gate\n",
    "      bias_i: a numpy array, bias for g gate\n",
    "      weight_xo: a numpy array, weight for output gate\n",
    "      weight_ho: a numpy array, weight for output gate\n",
    "      bias_o: a numpy array, bias for output gate\n",
    "    Return:\n",
    "      h : a numpy array, hidden state\n",
    "      c: a numpy array, context memory\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute f_t, i_t, g_t, o_t as defined above\n",
    "    ## START CODE HERE ## (~ 4 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    f_t = np.dot(weight_xf, x) + np.dot(weight_hf, h) + bias_f\n",
    "    i_t = np.dot(weight_xi, x) + np.dot(weight_hi, h) + bias_i\n",
    "    g_t = np.dot(weight_xg, x) + np.dot(weight_hg, h) + bias_g\n",
    "    o_t = np.dot(weight_xo, x) + np.dot(weight_ho, h) + bias_o\n",
    "    ## END CODE HERE ##\n",
    "\n",
    "    # Compute c_t\n",
    "    ## START CODE HERE ## (1 ~ 3 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    c_t = np.multiply(sigmoid(f_t), c)  + np.multiply(sigmoid(i_t), tanh(g_t))\n",
    "    ## END CODE HERE ##\n",
    "\n",
    "    # Compute h_t\n",
    "    ## START CODE HERE ## (1 ~ 2 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    h_t = np.multiply(sigmoid(o_t), tanh(c_t)) \n",
    "    ## END CODE HERE ##\n",
    "\n",
    "    return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "necessary-louisville",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2cdb31bfb23a92f2d06e895aadac63d",
     "grade": false,
     "grade_id": "lstm_layer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lstm_layer(inputs, h, c,\n",
    "               weight_xi, weight_hi, bias_i,\n",
    "               weight_xf, weight_hf, bias_f,\n",
    "               weight_xl, weight_hl, bias_l,\n",
    "               weight_xo, weight_ho, bias_o):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      inputs: a numpy array, a sequence of inputs: x_1, x_2, ...\n",
    "      h: a numpy array, initialized hidden state\n",
    "      c: a numpy array, initialized context memory\n",
    "      weight_xi: a numpy array, weight for input gate\n",
    "      weight_hi: a numpy array, weight for input gate\n",
    "      bias_i: a numpy array, bias for input gate\n",
    "      weight_xf: a numpy array, weight for forget gate\n",
    "      weight_hf: a numpy array, weight for forget gate\n",
    "      bias_f: a numpy array, bias for forget gate\n",
    "      weight_xg: a numpy array, weight for g gate\n",
    "      weight_hg: a numpy array, weight for g gate\n",
    "      bias_i: a numpy array, bias for g gate\n",
    "      weight_xo: a numpy array, weight for output gate\n",
    "      weight_ho: a numpy array, weight for output gate\n",
    "      bias_o: a numpy array, bias for output gate\n",
    "    Return:\n",
    "      h : a numpy array, final hidden state which is the vector representation of the inputs\n",
    "    \"\"\"\n",
    "\n",
    "    for x in inputs:\n",
    "        h, c = lstm_cell(x, h, c,\n",
    "                    weight_xi, weight_hi, bias_i,\n",
    "                    weight_xf, weight_hf, bias_f,\n",
    "                    weight_xl, weight_hl, bias_l,\n",
    "                    weight_xo, weight_ho, bias_o)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bridal-despite",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3204d7c1c9ed7e08481c8a96f6d3269",
     "grade": true,
     "grade_id": "lstm_cell_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "def test_lstm_cell():\n",
    "    hidden_dim = 3\n",
    "    output_size = 1\n",
    "    input_size = 2\n",
    "    dtype = np.float\n",
    "\n",
    "    h = np.zeros((hidden_dim, output_size), dtype=dtype)\n",
    "    c = np.zeros((hidden_dim, output_size), dtype=dtype)\n",
    "\n",
    "    inputs = np.array([[[1], [1]], [[2], [2]], [[3], [3]]], dtype=dtype)\n",
    "    x = np.array([[1.0], [1.0]], dtype=dtype)\n",
    "\n",
    "    weight_xf = np.random.randn(hidden_dim, input_size)\n",
    "    weight_xi = np.random.randn(hidden_dim, input_size)\n",
    "    weight_xg = np.random.randn(hidden_dim, input_size)\n",
    "    weight_xo = np.random.randn(hidden_dim, input_size)\n",
    "\n",
    "    weight_hf = np.random.randn(hidden_dim, hidden_dim)\n",
    "    weight_hi = np.random.randn(hidden_dim, hidden_dim)\n",
    "    weight_hg = np.random.randn(hidden_dim, hidden_dim)\n",
    "    weight_ho = np.random.randn(hidden_dim, hidden_dim)\n",
    "\n",
    "    bias_f = np.random.randn(hidden_dim, output_size)\n",
    "    bias_i = np.random.randn(hidden_dim, output_size)\n",
    "    bias_g = np.random.randn(hidden_dim, output_size)\n",
    "    bias_o = np.random.randn(hidden_dim, output_size)\n",
    "\n",
    "    cell_h, cell_c = lstm_cell(x, h, c,\n",
    "                            weight_xi, weight_hi, bias_i,\n",
    "                            weight_xf, weight_hf, bias_f,\n",
    "                            weight_xg, weight_hg, bias_g,\n",
    "                            weight_xo, weight_ho, bias_o)\n",
    "\n",
    "    output_h = lstm_layer(inputs, h, c,\n",
    "                        weight_xi, weight_hi, bias_i,\n",
    "                        weight_xf, weight_hf, bias_f,\n",
    "                        weight_xg, weight_hg, bias_g,\n",
    "                        weight_xo, weight_ho, bias_o)\n",
    "\n",
    "    assert cell_h.shape == (hidden_dim, output_size)\n",
    "    assert cell_c.shape == (hidden_dim, output_size)\n",
    "    assert output_h.shape == (hidden_dim, output_size)\n",
    "    print(\"Pass\")\n",
    "    return\n",
    "\n",
    "test_lstm_cell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "moderate-companion",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cea5beeff081c2065fc9c6a14d76d52",
     "grade": true,
     "grade_id": "lstm_layer_test",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "def test_lstm_layer():\n",
    "    \"\"\"\n",
    "    Compare the output of Tensorflow LSTM cell with yours. The output shall be the same as long as\n",
    "    your codes are correct.\n",
    "    \"\"\"\n",
    "    hidden_dim = 3\n",
    "    output_size = 1\n",
    "    input_size = 2\n",
    "    dtype = np.float\n",
    "\n",
    "    # Define inputs\n",
    "    inputs = np.array([[[1], [1]], [[2], [2]], [[3], [3]], [[4], [4]]], dtype=dtype)\n",
    "    inputs = tf.constant(inputs)\n",
    "    inputs = tf.transpose(inputs, perm=[2, 0, 1])\n",
    "    # LSTM cell from Tensorflow\n",
    "    lstm = tf.keras.layers.LSTM(units=hidden_dim)\n",
    "    tf_lstm_output = lstm(inputs)\n",
    "\n",
    "    # Get weights and bias from LSTM.\n",
    "    # They will be fed into your LSTM layer, so both networks will output the same value\n",
    "    kernel, recurrent_kernel, bias = lstm.get_weights()\n",
    "    kernel = tf.transpose(kernel)\n",
    "    recurrent_kernel = tf.transpose(recurrent_kernel)\n",
    "\n",
    "    weight_xi, weight_xf, weight_xg, weight_xo = tf.split(kernel, 4, axis=0)\n",
    "    weight_hi, weight_hf, weight_hg, weight_ho = tf.split(recurrent_kernel, 4, axis=0)\n",
    "    bias_i, bias_f, bias_g, bias_o = tf.split(bias, 4, axis=0)\n",
    "    bias_i = np.expand_dims(bias_i, -1)\n",
    "    bias_f = np.expand_dims(bias_f, -1)\n",
    "    bias_g = np.expand_dims(bias_g, -1)\n",
    "    bias_o = np.expand_dims(bias_o, -1)\n",
    "\n",
    "    # Calculate the output of your LSTM layer\n",
    "    inputs = np.array([[[1], [1]], [[2], [2]], [[3], [3]], [[4], [4]]], dtype=dtype)\n",
    "    h = np.zeros((hidden_dim, output_size), dtype=dtype)\n",
    "    c = np.zeros((hidden_dim, output_size), dtype=dtype)\n",
    "    np_lstm_output = lstm_layer(inputs, h, c,\n",
    "                  weight_xi, weight_hi, bias_i,\n",
    "                  weight_xf, weight_hf, bias_f,\n",
    "                  weight_xg, weight_hg, bias_g,\n",
    "                  weight_xo, weight_ho, bias_o)\n",
    "\n",
    "    # Compare the reults\n",
    "    np_lstm_output = np.reshape(np_lstm_output, tf_lstm_output.shape)\n",
    "    assert np.allclose(tf_lstm_output.numpy(), np_lstm_output), \"The outputs from Tensorflow LSTM and yours are not equal.\"\n",
    "    print(\"Pass\")\n",
    "    return\n",
    "\n",
    "test_lstm_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-berlin",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13a99bae16961d5d7ac6bf47146c58fd",
     "grade": false,
     "grade_id": "dense_classifier",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 3: Implement text classification with Tensorflow simple dense layers (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "still-madagascar",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "401ff170435ab6f07ef28756b736a9af",
     "grade": false,
     "grade_id": "imdb_load",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /userhome/cs/adrianxu/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7e22ce875a4e1e89cedd6fb70e85d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a85f767b60c4ceaa43ce12bea55d018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling imdb_reviews-train.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling imdb_reviews-test.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling imdb_reviews-unsupervised.tfrecord...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /userhome/cs/adrianxu/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB data from tensorflow dataset\n",
    "train_data, validation_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=(\"train[:60%]\", \"train[60%:]\", \"test\"),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "royal-custom",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6deb6207f9ffc48b78e4b5792376a8f",
     "grade": false,
     "grade_id": "embedding_load",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "/tmp/tfhub_modules/74a841d6eb84e8d93d913d716fb5440d020cc291.lock.tmp5d678066668c469da1b6b1b9cbae3137; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-df566c48aadd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/nnlm-en-dim50/2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m embedding_layer = hub.KerasLayer(embedding_url, input_shape=[],\n\u001b[0;32m----> 5\u001b[0;31m                                  dtype=tf.string, trainable=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected a string, got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   is_hub_module_v1 = tf.io.gfile.exists(\n\u001b[1;32m     94\u001b[0m       native_module.get_module_proto_path(module_path))\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_hub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     return resolver.atomic_download(handle, download, module_dir,\n\u001b[0;32m---> 68\u001b[0;31m                                     self._lock_file_timeout_sec())\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_lock_file_timeout_sec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         tf_utils.atomic_write_string_to_file(lock_file, lock_contents,\n\u001b[0;32m--> 381\u001b[0;31m                                              overwrite=False)\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Must test condition again, since another process could have created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# the module and deleted the old lock file since last test.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_hub/tf_utils.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[0;34m(filename, contents, overwrite)\u001b[0m\n\u001b[1;32m     63\u001b[0m                    tf.compat.as_bytes(uuid.uuid4().hex))\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, file_content)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;34m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_prewrite_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                                            \"File isn't open for writing\")\n\u001b[1;32m     87\u001b[0m       self._writable_file = _pywrap_file_io.WritableFile(\n\u001b[0;32m---> 88\u001b[0;31m           compat.path_to_bytes(self.__name), compat.as_bytes(self.__mode))\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /tmp/tfhub_modules/74a841d6eb84e8d93d913d716fb5440d020cc291.lock.tmp5d678066668c469da1b6b1b9cbae3137; Permission denied"
     ]
    }
   ],
   "source": [
    "# Load pre-trained word embeddings\n",
    "# If you encounter certification error, please refer to https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org\n",
    "embedding_url = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "embedding_layer = hub.KerasLayer(embedding_url, input_shape=[],\n",
    "                                 dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-finance",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53d9e3bb1ba3157eedb5b335cef7e65e",
     "grade": false,
     "grade_id": "text_classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    \"\"\"\n",
    "    Text classifier with LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_layer, hidden_size=16, learning_rate=1e-2):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            embedding_layer: pre-trained word embedding layer from hub.KerasLayer\n",
    "            hidden_size: hidden units of dense layer\n",
    "            learning_rate: learning rate of optimizer\n",
    "        \"\"\"\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.create_model()\n",
    "        self.compile_model()\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        self.model = tf.keras.Sequential()\n",
    "        # Add pre-trained embedding layer\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "        # Add a dense layer with hidden size as self.hidden_size\n",
    "        # and relu activation function\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "        # Add a dense layer to project the hidden layer into\n",
    "        # output layer, which hidden units is 1\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "    def compile_model(self):\n",
    "        # compile your model with Adam optimizer, which learning rate is defined\n",
    "        # as self.learning_rate; loss as BinaryCrossentropy, and metrics as \"accuracy\"\n",
    "        ## START CODE HERE (Hint: self.model.compile(...))\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "    def train(self, train_data, validation_data,\n",
    "            batch_size=512, train_epochs=3):\n",
    "        history = self.model.fit(train_data.shuffle(10000).batch(batch_size),\n",
    "                        epochs=train_epochs,\n",
    "                        validation_data=validation_data.batch(batch_size),\n",
    "                        verbose=1)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-philadelphia",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f3150075efe90b16b93b75f90cce354",
     "grade": true,
     "grade_id": "dense_classifer_test_1",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_text_classifier_1():\n",
    "    classifier = TextClassifier(embedding_layer)\n",
    "    model = classifier.model\n",
    "    assert len(model.layers) == 3, \"There should be 3 layers\"\n",
    "    layer_1_config = model.layers[1].get_config()\n",
    "    assert model.layers[1].units == classifier.hidden_size, \"Hidden units for second layer should be {}\".format(classifier.hidden_size)\n",
    "    assert model.layers[1].activation == tf.keras.activations.relu, \"Activation for second layer should be relu\"\n",
    "    assert model.layers[-1].units == 1, \"Hidden units for last layer should be 1\"\n",
    "    print(\"Pass\")\n",
    "\n",
    "\n",
    "test_text_classifier_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-upset",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d25e66843b5052120bfed780a2c28303",
     "grade": true,
     "grade_id": "dense_classifider_test_2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_text_classifier_2():\n",
    "    classifier = TextClassifier(embedding_layer)\n",
    "    model = classifier.model\n",
    "    assert isinstance(model.optimizer, tf.keras.optimizers.Adam), \"The optimizer should be Adam\"\n",
    "    assert isinstance(model.loss, tf.keras.losses.BinaryCrossentropy) or model.loss == \"binary_crossentropy\"\n",
    "    print(\"Pass\")\n",
    "\n",
    "\n",
    "test_text_classifier_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-colors",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f40c604ea481f0dd42dc7ee0e3db03e7",
     "grade": false,
     "grade_id": "dense_classifier_summary",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Summary the parameters of the model\n",
    "# Note: You can try differnt learning rate to train the model,\n",
    "# and find out how it affect the model performance.\n",
    "classifier = TextClassifier(embedding_layer)\n",
    "classifier.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-grenada",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e156906dbd656cafc12b3ab85403f774",
     "grade": false,
     "grade_id": "dense_classifier_train",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# To train the model, you can copy the following codes and paste them in\n",
    "# another cell. Please delete the added cell when you are ready to submit\n",
    "# you assignment.\n",
    "\n",
    "# history = classifier.train(train_data, validation_data)\n",
    "# plt.plot(history.history[\"accuracy\"])\n",
    "# plt.plot(history.history[\"val_accuracy\"])\n",
    "# plt.title(\"Model accuracy\")\n",
    "# plt.ylabel(\"accuracy\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.legend([\"train\", \"val\"], loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-poker",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a3cff2cd9068555b52cffca45683538",
     "grade": false,
     "grade_id": "bert_classifer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 4: Implement text classification with BERT (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-liberia",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea17d42376f25b9bf5b88a704fcc7096",
     "grade": false,
     "grade_id": "bert_loader",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load text processor and pre-trained weights for BERT\n",
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-basketball",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3279b8d5f2ed7c63162e2d4eff705f4a",
     "grade": false,
     "grade_id": "bert_classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class BertTextClassifier():\n",
    "    \"\"\"\n",
    "    Text classifier with BERT pre-trained model\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.1, learning_rate=1e-4):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.create_model()\n",
    "        self.compile_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        # Define the model input as string\n",
    "        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
    "        # BERT tokenization\n",
    "        preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=\"preprocessing\")\n",
    "        encoder_inputs = preprocessing_layer(text_input)\n",
    "        # BERT encodes the text into hidden values\n",
    "        encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name=\"bert_encoder\")\n",
    "        outputs = encoder(encoder_inputs)\n",
    "        # Get pooled_output from outputs\n",
    "        out = outputs[\"pooled_output\"]\n",
    "\n",
    "        # Dropout layer with dropout rate.\n",
    "        # Hint: out = dropout_layer(out)\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "        # Fully connected layer\n",
    "        # Fully-connected layer with hidden units 1.\n",
    "        # Hint: out = fully_connected(out)\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "        # Return output\n",
    "        self.model = tf.keras.Model(text_input, out)\n",
    "\n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(self.learning_rate),\n",
    "                           loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                           metrics=[\"accuracy\"])\n",
    "\n",
    "    def train(self, train_ds, validation_ds, epochs=3):\n",
    "        history = self.model.fit(x=train_ds.shuffle(1000).batch(16),\n",
    "                                 validation_data=validation_ds.batch(16),\n",
    "                                 epochs=epochs)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-snapshot",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d786e91f4e46844ff1f1a40b7f5de72e",
     "grade": true,
     "grade_id": "bert_classifier_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bert_classifier = BertTextClassifier()\n",
    "def test_bert_text_classifer():\n",
    "    model = bert_classifier.model\n",
    "    assert len(model.layers) == 5, \"The model should have 5 layers\"\n",
    "    assert isinstance(model.layers[3], tf.keras.layers.Dropout), \"There is no dropout layer\"\n",
    "    assert isinstance(model.layers[4], tf.keras.layers.Dense), \"There is no dense layer\"\n",
    "    assert model.layers[4].units == 1\n",
    "    print(\"Pass\")\n",
    "\n",
    "test_bert_text_classifer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-shore",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "455776f8127b7137e4ca61ef5551e378",
     "grade": false,
     "grade_id": "bert_classifier_train",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train bert model\n",
    "# You may need GPUs for this training.\n",
    "# Follow the guidelines as stated in Assignment one\n",
    "# Run the following line in another cell when needed\n",
    "# Please be remined that the added cells should be\n",
    "# removed when you submit your assignment\n",
    "\n",
    "# history = bert_classifier.train(train_data, validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
