{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeea200157319b12930300a5b5465bb1",
     "grade": false,
     "grade_id": "des",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: text classification.\n",
    "In this notebook assignment, you will use **Tensorflow 2** to build a deep neural network for a simple text classification task. Specifically, you will\n",
    "- build a simple LSTM network with basic **numpy** functions;\n",
    "- trains a sentiment analysis model to classify movie reviews [IMDB](https://ai.stanford.edu/%7Eamaas/data/sentiment/) as positive or negative, based on the text of the review. You will implement a deep neural network, with the advantage of pre-trained word embedding and BERT model.\n",
    "\n",
    "## Important!!\n",
    "- Please finish Problem 1 to 4. Fill the codes between **STAET CODE HERE** and **END CODE HERE**.\n",
    "- Make sure you pass the tester functions before your submission. Feel free to add cells to debug your codes.\n",
    "- **Please rename your sumited jupyter notebok with your student ID**. Otherwise, your work can not be scored accordingly.\n",
    "- You can add cells when you are preparing your assignment. DO **delete** them when you are ready to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a65538c255a834e63ac23379e2a9bc2",
     "grade": false,
     "grade_id": "installation",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip3 install -q tensorflow\n",
    "!pip3 install -q tensorflow-text\n",
    "!pip3 install -q tensorflow_datasets\n",
    "!pip3 install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "067a3c04224c8d41c8046b176a0fa3e0",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\"\n",
    "assert tf.__version__.split(\".\")[0] == \"2\", \"Your version of Tensorflow is not version 2.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f82af955031d5099db32355640383e82",
     "grade": false,
     "grade_id": "activation_des",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem :1 activation function (10 points) \n",
    "\n",
    "Implement **tanh** and **sigmoid** activatin function with **numpy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d342a3cae799bf6e58356c65bd31096",
     "grade": false,
     "grade_id": "activation_tanh",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: a numpy array\n",
    "    Return:\n",
    "    s : a numpy array\n",
    "    \"\"\"\n",
    "    ## START CODE HERE ## (~ 1 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ## END CODE HERE ##\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd2554e37790c0fd5db55920b0de30ee",
     "grade": false,
     "grade_id": "activation_sigmoid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: a numpy array\n",
    "    Return:\n",
    "    s : a numpy array\n",
    "    \"\"\"\n",
    "    ## START CODE HERE ## (~ 1 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ## END CODE HERE ##\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "713b7cb8a3ce1760c52858d7c212f01d",
     "grade": true,
     "grade_id": "activation_tanh_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_tanh(x):\n",
    "    assert isinstance(x, np.ndarray) is True, \"x should be of type numpy.ndarray\"\n",
    "    print(\"Pass\")\n",
    "    return\n",
    "\n",
    "x = np.array([0.5], dtype=np.float)\n",
    "test_tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad16c43730f76ba685cfacf6960fbbf",
     "grade": true,
     "grade_id": "activation_sigmoid_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_sigmoid(x):\n",
    "    assert isinstance(x, np.ndarray) is True, \"x should be of type numpy.ndarray\"\n",
    "    print(\"Pass\")\n",
    "    return\n",
    "x = np.array([0.5], dtype=np.float)\n",
    "test_sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fee6b4c6a1429f8ae4790cd0e6be7bc",
     "grade": false,
     "grade_id": "lstm_des",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 2: LSTM cell (40 points)\n",
    "LSTM Cell computes $c$, and $h$. $c$ is like the long-term memory, and $h$ is like the short term memory. We use the input $x$ and $h$ to update the long term memory. In the update, some features of $c$ are cleared with a forget gate $f$, and some features $i$ are added through a gate $g$.\n",
    "\n",
    "Here is the update rule:\n",
    "$$c_t = \\sigma(f_t) \\bigodot c_{t-1} + \\sigma(i_t) \\bigodot tanh(g_t)$$\n",
    "\n",
    "$$h_t = \\sigma(o_t) \\bigodot tanh(c_t)$$\n",
    "\n",
    "$\\bigodot$ stands for element-wise multiplication.\n",
    "\n",
    "Intermediate values and gates are computed as linear transformations of the hidden state and input.\n",
    "\n",
    "$$f_t = W_{xf} x_t + W_{hf} h_{t-1} + b_f$$\n",
    "\n",
    "$$i_t = W_{xi} x_t + W_{hi} h_{t-1} + b_i$$\n",
    "\n",
    "$$g_t = W_{xg} x_t + W_{hg} h_{t-1} + b_g$$\n",
    "\n",
    "$$o_t = W_{xo} x_t + W_{ho} h_{t-1} + b_o$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "531eb633fd4c22611249760ef2c56946",
     "grade": false,
     "grade_id": "lstm_cell",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lstm_cell(x, h, c,\n",
    "              weight_xi, weight_hi, bias_i,\n",
    "              weight_xf, weight_hf, bias_f,\n",
    "              weight_xg, weight_hg, bias_g,\n",
    "              weight_xo, weight_ho, bias_o):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      x: a numpy array, input state\n",
    "      h: a numpy array, hidden state\n",
    "      c: a numpy array, context memory\n",
    "      weight_xi: a numpy array, weight for input gate\n",
    "      weight_hi: a numpy array, weight for input gate\n",
    "      bias_i: a numpy array, bias for input gate\n",
    "      weight_xf: a numpy array, weight for forget gate\n",
    "      weight_hf: a numpy array, weight for forget gate\n",
    "      bias_f: a numpy array, bias for forget gate\n",
    "      weight_xg: a numpy array, weight for g gate\n",
    "      weight_hg: a numpy array, weight for g gate\n",
    "      bias_i: a numpy array, bias for g gate\n",
    "      weight_xo: a numpy array, weight for output gate\n",
    "      weight_ho: a numpy array, weight for output gate\n",
    "      bias_o: a numpy array, bias for output gate\n",
    "    Return:\n",
    "      h : a numpy array, hidden state\n",
    "      c: a numpy array, context memory\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute f_t, i_t, g_t, o_t as defined above\n",
    "    ## START CODE HERE ## (~ 4 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ## END CODE HERE ##\n",
    "\n",
    "    # Compute c_t\n",
    "    ## START CODE HERE ## (1 ~ 3 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ## END CODE HERE ##\n",
    "\n",
    "    # Compute h_t\n",
    "    ## START CODE HERE ## (1 ~ 2 line of code)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ## END CODE HERE ##\n",
    "\n",
    "    return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2cdb31bfb23a92f2d06e895aadac63d",
     "grade": false,
     "grade_id": "lstm_layer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lstm_layer(inputs, h, c,\n",
    "               weight_xi, weight_hi, bias_i,\n",
    "               weight_xf, weight_hf, bias_f,\n",
    "               weight_xl, weight_hl, bias_l,\n",
    "               weight_xo, weight_ho, bias_o):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      inputs: a numpy array, a sequence of inputs: x_1, x_2, ...\n",
    "      h: a numpy array, initialized hidden state\n",
    "      c: a numpy array, initialized context memory\n",
    "      weight_xi: a numpy array, weight for input gate\n",
    "      weight_hi: a numpy array, weight for input gate\n",
    "      bias_i: a numpy array, bias for input gate\n",
    "      weight_xf: a numpy array, weight for forget gate\n",
    "      weight_hf: a numpy array, weight for forget gate\n",
    "      bias_f: a numpy array, bias for forget gate\n",
    "      weight_xg: a numpy array, weight for g gate\n",
    "      weight_hg: a numpy array, weight for g gate\n",
    "      bias_i: a numpy array, bias for g gate\n",
    "      weight_xo: a numpy array, weight for output gate\n",
    "      weight_ho: a numpy array, weight for output gate\n",
    "      bias_o: a numpy array, bias for output gate\n",
    "    Return:\n",
    "      h : a numpy array, final hidden state which is the vector representation of the inputs\n",
    "    \"\"\"\n",
    "\n",
    "    for x in inputs:\n",
    "        h, c = lstm_cell(x, h, c,\n",
    "                    weight_xi, weight_hi, bias_i,\n",
    "                    weight_xf, weight_hf, bias_f,\n",
    "                    weight_xl, weight_hl, bias_l,\n",
    "                    weight_xo, weight_ho, bias_o)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3204d7c1c9ed7e08481c8a96f6d3269",
     "grade": true,
     "grade_id": "lstm_cell_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_lstm_cell():\n",
    "    hidden_dim = 3\n",
    "    output_size = 1\n",
    "    input_size = 2\n",
    "    dtype = np.float\n",
    "\n",
    "    h = np.zeros((hidden_dim, output_size), dtype=dtype)\n",
    "    c = np.zeros((hidden_dim, output_size), dtype=dtype)\n",
    "\n",
    "    inputs = np.array([[[1], [1]], [[2], [2]], [[3], [3]]], dtype=dtype)\n",
    "    x = np.array([[1.0], [1.0]], dtype=dtype)\n",
    "\n",
    "    weight_xf = np.random.randn(hidden_dim, input_size)\n",
    "    weight_xi = np.random.randn(hidden_dim, input_size)\n",
    "    weight_xg = np.random.randn(hidden_dim, input_size)\n",
    "    weight_xo = np.random.randn(hidden_dim, input_size)\n",
    "\n",
    "    weight_hf = np.random.randn(hidden_dim, hidden_dim)\n",
    "    weight_hi = np.random.randn(hidden_dim, hidden_dim)\n",
    "    weight_hg = np.random.randn(hidden_dim, hidden_dim)\n",
    "    weight_ho = np.random.randn(hidden_dim, hidden_dim)\n",
    "\n",
    "    bias_f = np.random.randn(hidden_dim, output_size)\n",
    "    bias_i = np.random.randn(hidden_dim, output_size)\n",
    "    bias_g = np.random.randn(hidden_dim, output_size)\n",
    "    bias_o = np.random.randn(hidden_dim, output_size)\n",
    "\n",
    "    cell_h, cell_c = lstm_cell(x, h, c,\n",
    "                            weight_xi, weight_hi, bias_i,\n",
    "                            weight_xf, weight_hf, bias_f,\n",
    "                            weight_xg, weight_hg, bias_g,\n",
    "                            weight_xo, weight_ho, bias_o)\n",
    "\n",
    "    output_h = lstm_layer(inputs, h, c,\n",
    "                        weight_xi, weight_hi, bias_i,\n",
    "                        weight_xf, weight_hf, bias_f,\n",
    "                        weight_xg, weight_hg, bias_g,\n",
    "                        weight_xo, weight_ho, bias_o)\n",
    "\n",
    "    assert cell_h.shape == (hidden_dim, output_size)\n",
    "    assert cell_c.shape == (hidden_dim, output_size)\n",
    "    assert output_h.shape == (hidden_dim, output_size)\n",
    "    print(\"Pass\")\n",
    "    return\n",
    "\n",
    "test_lstm_cell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cea5beeff081c2065fc9c6a14d76d52",
     "grade": true,
     "grade_id": "lstm_layer_test",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_lstm_layer():\n",
    "    \"\"\"\n",
    "    Compare the output of Tensorflow LSTM cell with yours. The output shall be the same as long as\n",
    "    your codes are correct.\n",
    "    \"\"\"\n",
    "    hidden_dim = 3\n",
    "    output_size = 1\n",
    "    input_size = 2\n",
    "    dtype = np.float\n",
    "\n",
    "    # Define inputs\n",
    "    inputs = np.array([[[1], [1]], [[2], [2]], [[3], [3]], [[4], [4]]], dtype=dtype)\n",
    "    inputs = tf.constant(inputs)\n",
    "    inputs = tf.transpose(inputs, perm=[2, 0, 1])\n",
    "    # LSTM cell from Tensorflow\n",
    "    lstm = tf.keras.layers.LSTM(units=hidden_dim)\n",
    "    tf_lstm_output = lstm(inputs)\n",
    "\n",
    "    # Get weights and bias from LSTM.\n",
    "    # They will be fed into your LSTM layer, so both networks will output the same value\n",
    "    kernel, recurrent_kernel, bias = lstm.get_weights()\n",
    "    kernel = tf.transpose(kernel)\n",
    "    recurrent_kernel = tf.transpose(recurrent_kernel)\n",
    "\n",
    "    weight_xi, weight_xf, weight_xg, weight_xo = tf.split(kernel, 4, axis=0)\n",
    "    weight_hi, weight_hf, weight_hg, weight_ho = tf.split(recurrent_kernel, 4, axis=0)\n",
    "    bias_i, bias_f, bias_g, bias_o = tf.split(bias, 4, axis=0)\n",
    "    bias_i = np.expand_dims(bias_i, -1)\n",
    "    bias_f = np.expand_dims(bias_f, -1)\n",
    "    bias_g = np.expand_dims(bias_g, -1)\n",
    "    bias_o = np.expand_dims(bias_o, -1)\n",
    "\n",
    "    # Calculate the output of your LSTM layer\n",
    "    inputs = np.array([[[1], [1]], [[2], [2]], [[3], [3]], [[4], [4]]], dtype=dtype)\n",
    "    h = np.zeros((hidden_dim, output_size), dtype=dtype)\n",
    "    c = np.zeros((hidden_dim, output_size), dtype=dtype)\n",
    "    np_lstm_output = lstm_layer(inputs, h, c,\n",
    "                  weight_xi, weight_hi, bias_i,\n",
    "                  weight_xf, weight_hf, bias_f,\n",
    "                  weight_xg, weight_hg, bias_g,\n",
    "                  weight_xo, weight_ho, bias_o)\n",
    "\n",
    "    # Compare the reults\n",
    "    np_lstm_output = np.reshape(np_lstm_output, tf_lstm_output.shape)\n",
    "    assert np.allclose(tf_lstm_output.numpy(), np_lstm_output), \"The outputs from Tensorflow LSTM and yours are not equal.\"\n",
    "    print(\"Pass\")\n",
    "    return\n",
    "\n",
    "test_lstm_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13a99bae16961d5d7ac6bf47146c58fd",
     "grade": false,
     "grade_id": "dense_classifier",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 3: Implement text classification with Tensorflow simple dense layers (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "401ff170435ab6f07ef28756b736a9af",
     "grade": false,
     "grade_id": "imdb_load",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load IMDB data from tensorflow dataset\n",
    "train_data, validation_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=(\"train[:60%]\", \"train[60%:]\", \"test\"),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6deb6207f9ffc48b78e4b5792376a8f",
     "grade": false,
     "grade_id": "embedding_load",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings\n",
    "# If you encounter certification error, please refer to https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org\n",
    "embedding_url = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "embedding_layer = hub.KerasLayer(embedding_url, input_shape=[],\n",
    "                                 dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53d9e3bb1ba3157eedb5b335cef7e65e",
     "grade": false,
     "grade_id": "text_classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    \"\"\"\n",
    "    Text classifier with LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_layer, hidden_size=16, learning_rate=1e-2):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            embedding_layer: pre-trained word embedding layer from hub.KerasLayer\n",
    "            hidden_size: hidden units of dense layer\n",
    "            learning_rate: learning rate of optimizer\n",
    "        \"\"\"\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.create_model()\n",
    "        self.compile_model()\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        self.model = tf.keras.Sequential()\n",
    "        # Add pre-trained embedding layer\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "        # Add a dense layer with hidden size as self.hidden_size\n",
    "        # and relu activation function\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "        # Add a dense layer to project the hidden layer into\n",
    "        # output layer, which hidden units is 1\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "    def compile_model(self):\n",
    "        # compile your model with Adam optimizer, which learning rate is defined\n",
    "        # as self.learning_rate; loss as BinaryCrossentropy, and metrics as \"accuracy\"\n",
    "        ## START CODE HERE (Hint: self.model.compile(...))\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "    def train(self, train_data, validation_data,\n",
    "            batch_size=512, train_epochs=3):\n",
    "        history = self.model.fit(train_data.shuffle(10000).batch(batch_size),\n",
    "                        epochs=train_epochs,\n",
    "                        validation_data=validation_data.batch(batch_size),\n",
    "                        verbose=1)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f3150075efe90b16b93b75f90cce354",
     "grade": true,
     "grade_id": "dense_classifer_test_1",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_text_classifier_1():\n",
    "    classifier = TextClassifier(embedding_layer)\n",
    "    model = classifier.model\n",
    "    assert len(model.layers) == 3, \"There should be 3 layers\"\n",
    "    layer_1_config = model.layers[1].get_config()\n",
    "    assert model.layers[1].units == classifier.hidden_size, \"Hidden units for second layer should be {}\".format(classifier.hidden_size)\n",
    "    assert model.layers[1].activation == tf.keras.activations.relu, \"Activation for second layer should be relu\"\n",
    "    assert model.layers[-1].units == 1, \"Hidden units for last layer should be 1\"\n",
    "    print(\"Pass\")\n",
    "\n",
    "\n",
    "test_text_classifier_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d25e66843b5052120bfed780a2c28303",
     "grade": true,
     "grade_id": "dense_classifider_test_2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_text_classifier_2():\n",
    "    classifier = TextClassifier(embedding_layer)\n",
    "    model = classifier.model\n",
    "    assert isinstance(model.optimizer, tf.keras.optimizers.Adam), \"The optimizer should be Adam\"\n",
    "    assert isinstance(model.loss, tf.keras.losses.BinaryCrossentropy) or model.loss == \"binary_crossentropy\"\n",
    "    print(\"Pass\")\n",
    "\n",
    "\n",
    "test_text_classifier_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f40c604ea481f0dd42dc7ee0e3db03e7",
     "grade": false,
     "grade_id": "dense_classifier_summary",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Summary the parameters of the model\n",
    "# Note: You can try differnt learning rate to train the model,\n",
    "# and find out how it affect the model performance.\n",
    "classifier = TextClassifier(embedding_layer)\n",
    "classifier.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e156906dbd656cafc12b3ab85403f774",
     "grade": false,
     "grade_id": "dense_classifier_train",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# To train the model, you can copy the following codes and paste them in\n",
    "# another cell. Please delete the added cell when you are ready to submit\n",
    "# you assignment.\n",
    "\n",
    "# history = classifier.train(train_data, validation_data)\n",
    "# plt.plot(history.history[\"accuracy\"])\n",
    "# plt.plot(history.history[\"val_accuracy\"])\n",
    "# plt.title(\"Model accuracy\")\n",
    "# plt.ylabel(\"accuracy\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.legend([\"train\", \"val\"], loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a3cff2cd9068555b52cffca45683538",
     "grade": false,
     "grade_id": "bert_classifer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 4: Implement text classification with BERT (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea17d42376f25b9bf5b88a704fcc7096",
     "grade": false,
     "grade_id": "bert_loader",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load text processor and pre-trained weights for BERT\n",
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3279b8d5f2ed7c63162e2d4eff705f4a",
     "grade": false,
     "grade_id": "bert_classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class BertTextClassifier():\n",
    "    \"\"\"\n",
    "    Text classifier with BERT pre-trained model\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.1, learning_rate=1e-4):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.create_model()\n",
    "        self.compile_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        # Define the model input as string\n",
    "        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
    "        # BERT tokenization\n",
    "        preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=\"preprocessing\")\n",
    "        encoder_inputs = preprocessing_layer(text_input)\n",
    "        # BERT encodes the text into hidden values\n",
    "        encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name=\"bert_encoder\")\n",
    "        outputs = encoder(encoder_inputs)\n",
    "        # Get pooled_output from outputs\n",
    "        out = outputs[\"pooled_output\"]\n",
    "\n",
    "        # Dropout layer with dropout rate.\n",
    "        # Hint: out = dropout_layer(out)\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "        # Fully connected layer\n",
    "        # Fully-connected layer with hidden units 1.\n",
    "        # Hint: out = fully_connected(out)\n",
    "        ## START CODE HERE ## (~1 line of code)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ## END CODE HERE\n",
    "\n",
    "        # Return output\n",
    "        self.model = tf.keras.Model(text_input, out)\n",
    "\n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(self.learning_rate),\n",
    "                           loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                           metrics=[\"accuracy\"])\n",
    "\n",
    "    def train(self, train_ds, validation_ds, epochs=3):\n",
    "        history = self.model.fit(x=train_ds.shuffle(1000).batch(16),\n",
    "                                 validation_data=validation_ds.batch(16),\n",
    "                                 epochs=epochs)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d786e91f4e46844ff1f1a40b7f5de72e",
     "grade": true,
     "grade_id": "bert_classifier_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bert_classifier = BertTextClassifier()\n",
    "def test_bert_text_classifer():\n",
    "    model = bert_classifier.model\n",
    "    assert len(model.layers) == 5, \"The model should have 5 layers\"\n",
    "    assert isinstance(model.layers[3], tf.keras.layers.Dropout), \"There is no dropout layer\"\n",
    "    assert isinstance(model.layers[4], tf.keras.layers.Dense), \"There is no dense layer\"\n",
    "    assert model.layers[4].units == 1\n",
    "    print(\"Pass\")\n",
    "\n",
    "test_bert_text_classifer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "455776f8127b7137e4ca61ef5551e378",
     "grade": false,
     "grade_id": "bert_classifier_train",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train bert model\n",
    "# You may need GPUs for this training.\n",
    "# Follow the guidelines as stated in Assignment one\n",
    "# Run the following line in another cell when needed\n",
    "# Please be remined that the added cells should be\n",
    "# removed when you submit your assignment\n",
    "\n",
    "# history = bert_classifier.train(train_data, validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
